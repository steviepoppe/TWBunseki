"""
Get different metrics for CSV file generated by twitter_search.py

Run:
- Install requirements (`$ pip install pandas`)
- Have csv file ready (generated by twitter_search.py)
- Run `$ python get_metrics.py --help` for exact arguments
"""

import argparse
import csv
from datetime import date, datetime
from dateutil import parser
import json
from pathlib import Path
import re
import requests
import sys
import time

import pandas as pd


def parse_tweets(args):

	file_name = args['filename']
	timezone = args['timezone']  # example: 'Asia/Tokyo', 'UTC'
	keep_rt = not args['no_keep_rt']
	analyze_datetime = not args['no_analyze_datetime']
	analyze_users = not args['no_analyze_users']
	analyze_hashtags = not args['no_analyze_hashtags']
	analyze_urls = args['analyze_urls']
	exclude_twitter_urls = args['exclude_twitter_urls']
	chunksize = args['chunk_size']
	hashtags = {}
	hashtag_dates = {}
	date_set = {}
	time_set = {}
	user_set = {}
	media_set = {}
	line_count = 0
	
	Path("./results/metrics_%s/" % file_name).mkdir(parents=True, exist_ok=True)

	for chunk in pd.read_csv('./results/%s.csv' % file_name, encoding="utf-8", chunksize=chunksize, iterator=True):
		if timezone is not None:
			chunk.created_at = pd.to_datetime(chunk.created_at, utc=True)
			chunk.created_at = chunk.created_at.dt.tz_convert(tz=timezone)
			chunk.created_at = chunk.created_at.apply(str)
		for index, tweet in chunk.iterrows():
			line_count += 1
			is_retweet = 1 if tweet["is_retweet"] == True else 0
			if analyze_hashtags:
				hashtag_metrics(tweet, hashtags, is_retweet)
				hashtag_date_metrics(tweet, hashtag_dates, is_retweet)
			if analyze_datetime:
				date_metrics(tweet, date_set, is_retweet)
				time_metrics(tweet, time_set, is_retweet)
			if analyze_users:
				user_metrics(tweet, user_set, is_retweet, keep_rt)
			if analyze_urls:
				media_metrics(tweet, media_set, is_retweet, exclude_twitter_urls)
		print('Processed %s lines.' % line_count)
		
	print('Processed total of %s lines.' % line_count)
	if analyze_hashtags:
		save_hashtag_metrics(hashtags, file_name)
		save_hashtag_date_metrics(hashtag_dates, file_name)
	if analyze_datetime:
		save_date_metrics(date_set, file_name)
		save_time_metrics(time_set, file_name)
	if analyze_users:
		save_user_metrics(user_set, file_name)
	if analyze_urls:
		save_media_metrics(media_set, file_name)
			
def hashtag_metrics(tweet, hashtags, is_retweet):
	if not pd.isna(tweet["hashtags"]):
		c_hashtags = tweet["hashtags"].replace(",,",",").split(",") # possibility of empty strings joined = two commas
		for hashtag in c_hashtags:						
			if hashtag != '':
				if hashtag in hashtags:
					hashtags[hashtag][is_retweet] = hashtags[hashtag][is_retweet] + 1
				else:
					retweet_status = [0,0,0]
					retweet_status[is_retweet] = 1
					retweet_status[not is_retweet] = 0
					retweet_status[2] = [0,0]
					retweet_status[2][0] = []
					retweet_status[2][1] = []
					hashtags[hashtag] = retweet_status
				hashtags[hashtag][2][is_retweet].append(tweet["user_screen_name"])


def hashtag_date_metrics(tweet, hashtag_dates, is_retweet):
	tweet_created_month = parser.parse(tweet["created_at"]).strftime("%m/%Y") # month/year
	if not pd.isna(tweet["hashtags"]):
		c_hashtags = tweet["hashtags"].replace(",,",",").split(",") # possibility of empty strings joined = two commas
		for hashtag in c_hashtags:						
			if hashtag != '':
				if hashtag_dates.get(hashtag, {}).get(tweet_created_month) is not None:
					hashtag_dates[hashtag][tweet_created_month][is_retweet] = hashtag_dates[hashtag][tweet_created_month][is_retweet] + 1
				else:
					retweet_status = [0,0,0]
					retweet_status[is_retweet] = 1
					retweet_status[not is_retweet] = 0
					retweet_status[2] = [0,0]
					retweet_status[2][0] = []
					retweet_status[2][1] = []
					if hashtag not in hashtag_dates:
						hashtag_dates[hashtag] = {}
					hashtag_dates[hashtag][tweet_created_month] = retweet_status
				hashtag_dates[hashtag][tweet_created_month][2][is_retweet].append(tweet["user_screen_name"])

def date_metrics(tweet, date_set, is_retweet):
	#print(tweet["created_at"])
	tweet_created_date = parser.parse(tweet["created_at"]).strftime("%m/%d/%Y")
#	print(tweet_created_date.strftime("%m/%d/%Y"))
#	tweet_created_date = datetime.fromisoformat(tweet_created_date).strftime("%m/%d/%Y")
	if not tweet_created_date in date_set:
		retweet_status = [0,0,0]
		retweet_status[is_retweet] = 1
		retweet_status[not is_retweet] = 0
		date_set[tweet_created_date] = retweet_status
		retweet_status[2] = [0,0]
		retweet_status[2][0] = []
		retweet_status[2][1] = []
	else:
		date_set[tweet_created_date][is_retweet] += 1
	date_set[tweet_created_date][2][is_retweet].append(tweet["user_screen_name"])

def time_metrics(tweet, time_set, is_retweet):
	tweet_created_time = parser.parse(tweet["created_at"]).strftime("%H")  #change to %I %p for AM/PM

	if not tweet_created_time in time_set:
		retweet_status = [0,0,0]
		retweet_status[is_retweet] = 1
		retweet_status[not is_retweet] = 0
		time_set[tweet_created_time] = retweet_status
		retweet_status[2] = [0,0]
		retweet_status[2][0] = []
		retweet_status[2][1] = []
	else:
		time_set[tweet_created_time][is_retweet] += 1
	time_set[tweet_created_time][2][is_retweet].append(tweet["user_screen_name"])


def media_metrics(tweet, media_set, is_retweet, exclude_twitter_urls):
	pattern = re.compile(r'.*(https://t.co/.{10}).*')
	result = pattern.match(tweet['text'])
	if result is None:
		return
	
	url = result[1]
	print(f'Expanding URL {url}...')
	if url in media_set:
		media_set[url]['metrics'][is_retweet] += 1
	else:
		expanded = ''
		try:
			res = requests.get(url, allow_redirects=False)
			expanded = res.headers.get('location', '')
		except Exception:
			pass
		if exclude_twitter_urls and expanded.startswith('https://twitter.com/'):
			return
		media_set[url] = {}
		media_set[url]['error_expanding'] = expanded == ''
		media_set[url]['expanded'] = expanded
		media_set[url]['metrics'] = [0, 0]  # [tweets, retweets]
		media_set[url]['metrics'][is_retweet] = 1


def user_metrics(tweet, user_set, is_retweet, keep_rt):
	if ("retweeted_status" in tweet) == False or keep_rt == True:
		user = {}
		if tweet["user_screen_name"] not in user_set:
			user["screen_name"] = tweet["user_screen_name"]
			user["description"] = tweet["user_description"]
			user["following_count"] = tweet["user_following_count"]
			user["followers_count"] = tweet["user_followers_count"]
			user["total_tweets"] = tweet["user_total_tweets"]
			user["created_at"] = tweet["user_created_at"]
			user["total_in_data_set"] = [0,0]
			user["total_in_data_set"][is_retweet] = 1
			## Note, if full retweet count is preferred, replace "retweet_count_dataset" with "retweet_count_listed"
			if "retweet_count_dataset" in tweet:
				user["total_times_retweeted_in_dataset"] = int(tweet["retweet_count_dataset"])
			user_set[tweet["user_screen_name"]] = user

		else:			
			user_set[tweet["user_screen_name"]]["total_in_data_set"][is_retweet] += 1
			if "retweet_count_dataset" in tweet:
				user_set[tweet["user_screen_name"]]["total_times_retweeted_in_dataset"] += int(tweet["retweet_count_dataset"])
			if tweet["user_following_count"] > user_set[tweet["user_screen_name"]]["following_count"]:
				user_set[tweet["user_screen_name"]]["following_count"] 
			if tweet["user_followers_count"] > user_set[tweet["user_screen_name"]]["followers_count"]:
				user_set[tweet["user_screen_name"]]["followers_count"] 
			if tweet["user_total_tweets"] > user_set[tweet["user_screen_name"]]["total_tweets"]:
				user_set[tweet["user_screen_name"]]["total_tweets"] 

		#if tweet["user_screen_name"] not in unique_users[is_retweet]:
		#	unique_users[is_retweet].append(tweet["user_screen_name"])

def save_hashtag_metrics(hashtags, file_name):
	with open('./results/metrics_%s/%s_hashtags.csv' % (file_name, file_name), 
		mode='w', encoding="utf-8",newline='') as file_hashtags:
		writer_hashtags = csv.writer(file_hashtags)
		writer_hashtags.writerow(["hashtag","total_normal", "total_retweet", 
			"total","unique_tweeters", "re_unique_tweeters", "re_unique_tweeters_filtered", "total"])

		for hashtag, value in hashtags.items():
			normal = set(value[2][0])
			retweet = set(value[2][1])
			unique = [x for x in retweet if x not in normal]
			writer_hashtags.writerow([hashtag, value[0], value[1], value[0] + value[1], str(len(normal)), 
				str(len(retweet)), str(len(unique)), str(len(normal) + len(unique))])
	print("Finished. Saved to ./results/metrics_%s/%s_hashtags.csv" % (file_name, file_name))

def save_hashtag_date_metrics(hashtag_dates, file_name):
	with open('./results/metrics_%s/%s_hashtag_dates.csv' % (file_name, file_name), 
		mode='w', encoding="utf-8",newline='') as file_hashtags:
		writer_hashtags = csv.writer(file_hashtags)
		writer_hashtags.writerow(["hashtag", "month", "total_normal", "total_retweet", 
			"total","unique_tweeters", "re_unique_tweeters", "re_unique_tweeters_filtered", "total"])

		for hashtag, months in hashtag_dates.items():
			for month, value in months.items():
				normal = set(value[2][0])
				retweet = set(value[2][1])
				unique = [x for x in retweet if x not in normal]
				writer_hashtags.writerow([hashtag, month, value[0], value[1], value[0] + value[1], str(len(normal)), 
					str(len(retweet)), str(len(unique)), str(len(normal) + len(unique))])
	print("Finished. Saved to ./results/metrics_%s/%s_hashtag_dates.csv" % (file_name, file_name))

def save_date_metrics(date_set, file_name):
	with open('./results/metrics_%s/%s_date.csv' % (file_name, file_name), 
		mode='w', encoding="utf-8",newline='') as file_date:
		writer_date = csv.writer(file_date)
		writer_date.writerow(["date","total_normal", "total_retweet", "total_tweets","unique_normal_tweeters",
			"unique_retweeters_exist", "unique_retweeters_filtered", "unique_retweeters_total", 
			"total_tweeters"])

		for date, value in date_set.items():
			#unique_users[is_retweet]

			normal = set(value[2][0])
			retweet = set(value[2][1])
			unique = [x for x in retweet if x not in normal]
			writer_date.writerow([date, value[0], value[1], value[0] + value[1], 
				str(len(normal)), str(len(retweet) - len(unique)), str(len(unique)), str(len(retweet)), 
				str(len(normal) + len(unique))])

	print("Finished. Saved to ./results/metrics_%s/%s_date.csv" % (file_name, file_name))

def save_time_metrics(time_set, file_name):
	with open('./results/metrics_%s/%s_time.csv' % (file_name, file_name), 
		mode='w', encoding="utf-8",newline='') as file_time:
		writer_time = csv.writer(file_time)
		writer_time.writerow(["time","total_normal", "total_retweet", "total_tweets","unique_tweeters", 
			"re_unique_tweeters", "re_unique_tweeters_filtered", "total_tweeters"])

		for hour, value in time_set.items():
			normal = set(value[2][0])
			retweet = set(value[2][1])
			unique = [x for x in retweet if x not in normal]
			writer_time.writerow([hour, value[0], value[1], value[0] + value[1], 
				str(len(normal)), str(len(retweet)), str(len(unique)), str(len(normal) + len(retweet))])
	print("Finished. Saved to ./results/metrics_%s/%s_time.csv" % (file_name, file_name))

def save_media_metrics(media_set, file_name):
	with open('./results/metrics_%s/%s_media.csv' % (file_name, file_name),
		mode='w', encoding="utf-8", newline='') as file_media:
		writer_media = csv.writer(file_media)
		writer_media.writerow(['url', 'expanded_url', 'error_expanding', 'total_tweets', 'total_retweet'])

		for url, value in media_set.items():
			writer_media.writerow([url, value['expanded'], str(value['error_expanding']), str(value['metrics'][0]), str(value['metrics'][1])])
	print('Finished. Saved to ./results/metrics_%s/%s_media.csv' % (file_name, file_name))

def save_user_metrics(user_set, file_name):
	with open('./results/metrics_%s/%s_users.csv' % (file_name, file_name), 
		mode='w', encoding="utf-8",newline='') as file_users:
		writer_users = csv.writer(file_users)
			

		if "total_times_retweeted_in_dataset" in list(user_set.values())[0]:
			writer_users.writerow(["screen_name", "total_posted_normal","total_posted_retweets","total_posted", 
				"user_description","user_following_count", "user_followers_count", "user_total_tweets", 
				"total_times_retweeted_in_dataset","user_created_at"])
			with open('./results/metrics_%s/%s_old_retweets.csv' % (file_name, file_name), 
				mode='r', encoding="utf-8") as old_retweets:
				csv_reader = csv.DictReader(old_retweets)
				for tweet in csv_reader:
					if tweet["user_screen_name"] in user_set:
						user_set[tweet["user_screen_name"]]["total_times_retweeted_in_dataset"] += int(tweet["retweet_count"])
		else:
			writer_users.writerow(["screen_name", "total_posted_normal","total_posted_retweets","total_posted", 
				"user_description","user_following_count", "user_followers_count", "user_total_tweets","user_created_at"])

		for a in user_set:
			user = user_set[a]
			if "total_times_retweeted_in_dataset" not in user:
				writer_users.writerow([user["screen_name"],user["total_in_data_set"][0],user["total_in_data_set"][1], 
					(user["total_in_data_set"][0] + user["total_in_data_set"][1]), user["description"],
					user["following_count"],user["followers_count"],user["total_tweets"],user["created_at"]])
			else:
				writer_users.writerow([user["screen_name"],user["total_in_data_set"][0],user["total_in_data_set"][1], 
					(user["total_in_data_set"][0] + user["total_in_data_set"][1]), user["description"],
					user["following_count"],user["followers_count"],user["total_tweets"],
					user["total_times_retweeted_in_dataset"],user["created_at"]])

	print("Finished. Saved to ./results/metrics_%s/%s_users.csv" % (file_name, file_name))


if __name__ == '__main__':
	p = argparse.ArgumentParser(description='Analyze metrics for a Twitter corpus w/ format compatible with twitter_search.py')
	p.add_argument(
		'-f',
		'--filename',
		type=str,
		required=True,
		help='Name of the csv file inside ./results/ without .csv (e.g. MyResults would point to ./results/MyResults.csv)',
	)
	p.add_argument(
		'-c',
		'--chunk-size',
		type=int,
		default=100000,
		help='Size of processing chunk. Default: 100K rows'
	)
	p.add_argument(
		'-t',
		'--timezone',
		type=str,
		help='Timezone to convert time data to before analysis (does not impact original file) e.g. Asia/Tokyo (Optional)',
	)
	p.add_argument(
		'--analyze-urls',
		action='store_true',
		help='Use this to process/expand media/URLs'
	)
	p.add_argument(
		'--exclude-twitter-urls',
		action='store_true',
		help='Use this to exclude any url that expands to https://twitter.com/*'
	)
	p.add_argument(
		'--no-keep-rt',
	   	action='store_true',
		help='Use this to NOT process RTs',
	)
	p.add_argument(
		'--no-analyze-hashtags',
		action='store_true',
		help='Use this to NOT process hashtags'
	)
	p.add_argument(
		'--no-analyze-datetime',
		action='store_true',
		help='Use this to NOT process dates and times'
	)
	p.add_argument(
		'--no-analyze-users',
		action='store_true',
		help='Use this to NOT process users'
	)

	args = vars(p.parse_args())
	
	parse_tweets(args)
